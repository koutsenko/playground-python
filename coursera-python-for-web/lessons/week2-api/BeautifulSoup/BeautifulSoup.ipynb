{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Введение в Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Ответ, содержит заголовки, код статуса, куки, тело ответа и т.п.\n",
    "response = requests.get(\"https://wikipedia.org\")\n",
    "\n",
    "# Тело ответа в текстовом виде\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Поиск тэгов с использованием довольно сложной и ненадежной регулярки\n",
    "re.findall(r'<a[^>]*other-project-link[^>]*href=\"([^\"]*)', html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Поиск тэгов с использованием Beautiful Soup\n",
    "tags = soup('a', 'other-project-link')\n",
    "for tag in tags:\n",
    "    print(tag['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Такой же поиск тэгов, но используем списковое включение\n",
    "tags = [tag['href'] for tag in soup('a', 'other-project-link')]\n",
    "\n",
    "for tag in tags:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Обзор методов модуля Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Исходный HTML\n",
    "html = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "  <head>\n",
    "    <title>test page</title>\n",
    "  </head>\n",
    "  <body class=\"mybody\" id=\"js-body\">\n",
    "    <p class=\"text odd\">first <b>bold</b> paragraph</p>\n",
    "    <p class=\"text even\">second <a href=\"https://mail.ru\">link</a></p>\n",
    "    <p class=\"list odd\">third <a id=\"paragraph\"><b>bold link</b></a></p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Суп, распарсенный с помощью LXML\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Выведет текстовое представление супа\n",
    "print(soup)\n",
    "\n",
    "# Prettied-представление супа\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.p)                            # Обратимся к первому тэгу p в супе\n",
    "print(type(soup.p))                      # Тип данных тэга p - bs4.element.Tag\n",
    "print(type(soup.p.b))                    # Тип данных тэга b - bs4.element.Tag\n",
    "print(type(soup.p.b.string))             # Тип данных строки - bs4.element.NavigableString\n",
    "print('Строка:',   soup.p.b.string)      # Казалось бы выглядит как обычная строка\n",
    "print('Тэг:',      soup.b.name)          # Название тэга\n",
    "print('Тэг:',      soup.p.name)          # Название тэга\n",
    "print('Классы:',   soup.p['class'])      # Список классов (список или строка - зависит от спецификации атрибута)\n",
    "print('Классы:',   soup.body['class'])   # Список классов (даже если класс единственный)\n",
    "print('ID:',       soup.body['id'])      # ID body - всегда строка!\n",
    "print('Родитель:', soup.p.b.parent)      # Родитель у <b> это <p>\n",
    "print([t.name for t in soup.p.parents ]) # Родительские тэги\n",
    "print('Следующий:',soup.p.next)          # Следующий (со спуском)\n",
    "print('Сиблинг:',  soup.p.next_sibling)  # Выведет символ переноса строки, т.к. текст с переносами\n",
    "print('Содержимое',soup.p.contents)      # Список вложенных элементов\n",
    "print('Итератор:', soup.p.children)      # Список вложенных элементов (итератор)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Сложный поиск и изменение с Beautiful Soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.p.b.find_parent(id=\"js-body\").name)         # Найти не просто родителя, а родителя с id = \"js-body\"\n",
    "print(soup.p.b.find_parent(\"body\")['id'])              # Найти среди родителей тэг body и вывести его id\n",
    "print(soup.p.find_next_sibling(class_=\"odd\"))          # Найти сиблинга нужного типа и с нужным классом\n",
    "print(soup.p.find_next_siblings())                     # Список сиблингов \n",
    "print(soup.p.find('b'))                                # Найти тэг <b>\n",
    "print(soup.p.find('b', text='bold'))                   # Найти тэг <b> только такой который с текстом bold\n",
    "print(soup.find_all('p'))                              # Найти все тэги <p>\n",
    "print(soup.find_all('p', 'text odd'))                  # Поиск по тэгу и классу\n",
    "print(soup.find_all('p', 'odd text'))                  # Сменили порядок классов и ничего не находим....\n",
    "print(soup.find_all(name='p', class_='text odd'))      # CSS-селектор, поиск по классам\n",
    "print(soup.find_all(name='p', class_='odd text'))      # CSS-селектор, поиск по классам\n",
    "print(soup.select('p.odd'))                            # CSS-селектор, выборка всех p.odd\n",
    "print(soup.select('p:nth-of-type(3)'))                 # CSS-селектор, выведем третий тэг <p>\n",
    "print(soup.select('a > b'))                            # CSS-селектор, ищем прямого потомка\n",
    "\n",
    "import re\n",
    "[i.name for i in soup.find_all(name=re.compile('^b'))] # Все тэги начинающиеся с b\n",
    "[i for i in soup(['a', 'b'])]                          # Все тэги a и b - это уже без регулярок, просто список"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Редактируем!\n",
    "tag = soup.b\n",
    "tag.name = 'i'\n",
    "tag['id'] = 'myid'\n",
    "tag.string = 'italic'\n",
    "\n",
    "# Измененная верстка, можно сохранить например.\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Реальный пример - парсинг сайта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Соберем список секций и новостей в каждой из них\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "result = requests.get(\"https://news.mail.ru/\")\n",
    "html = result.text\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    (section.string, \n",
    "    [\n",
    "        link.string for link in section.find_parents()[4].find_all('span', 'link__text')\n",
    "    ] \n",
    "    ) for section in soup.find_all('span', 'hdr__inner')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание - песочница"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "======================================================================\n",
      "ERROR: test_build_bridge (__main__.TestGetStatistics) (path='wiki/', start_page='Stone_Age', end_page='Python_(programming_language)', expected=['Stone_Age', 'Brain', 'Artificial_intelligence', 'Python_(programming_language)'])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 112, in test_build_bridge\n",
      "    result = get_statistics(path, start_page, end_page)\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 88, in get_statistics\n",
      "    stats = parse(fpath)\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 17, in parse\n",
      "    headers_first = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
      "NameError: name 'body' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_build_bridge (__main__.TestGetStatistics) (path='wiki/', start_page='The_New_York_Times', end_page='Stone_Age', expected=['The_New_York_Times', 'London', 'Woolwich', 'Iron_Age', 'Stone_Age'])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 112, in test_build_bridge\n",
      "    result = get_statistics(path, start_page, end_page)\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 88, in get_statistics\n",
      "    stats = parse(fpath)\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 17, in parse\n",
      "    headers_first = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
      "NameError: name 'body' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_build_bridge (__main__.TestGetStatistics) (path='wiki/', start_page='Artificial_intelligence', end_page='Mei_Kurokawa', expected=['Artificial_intelligence', 'IBM', 'PlayStation_3', 'Wild_Arms_(video_game)', 'Hidamari_no_Ki', 'Mei_Kurokawa'])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 112, in test_build_bridge\n",
      "    result = get_statistics(path, start_page, end_page)\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 88, in get_statistics\n",
      "    stats = parse(fpath)\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 17, in parse\n",
      "    headers_first = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
      "NameError: name 'body' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_build_bridge (__main__.TestGetStatistics) (path='wiki/', start_page='The_New_York_Times', end_page='Binyamina_train_station_suicide_bombing', expected=['The_New_York_Times', 'Second_Intifada', 'Haifa_bus_16_suicide_bombing', 'Binyamina_train_station_suicide_bombing'])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 112, in test_build_bridge\n",
      "    result = get_statistics(path, start_page, end_page)\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 88, in get_statistics\n",
      "    stats = parse(fpath)\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 17, in parse\n",
      "    headers_first = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
      "NameError: name 'body' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_build_bridge (__main__.TestGetStatistics) (path='wiki/', start_page='Stone_Age', end_page='Stone_Age', expected=['Stone_Age'])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 112, in test_build_bridge\n",
      "    result = get_statistics(path, start_page, end_page)\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 88, in get_statistics\n",
      "    stats = parse(fpath)\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 17, in parse\n",
      "    headers_first = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
      "NameError: name 'body' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_parse (__main__.TestParse) (path='wiki/Stone_Age', expected=[13, 10, 12, 40])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 53, in test_parse\n",
      "    self.assertEqual(parse(path), expected)\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 17, in parse\n",
      "    headers_first = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
      "NameError: name 'body' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_parse (__main__.TestParse) (path='wiki/Brain', expected=[19, 5, 25, 11])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 53, in test_parse\n",
      "    self.assertEqual(parse(path), expected)\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 17, in parse\n",
      "    headers_first = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
      "NameError: name 'body' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_parse (__main__.TestParse) (path='wiki/Artificial_intelligence', expected=[8, 19, 13, 198])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 53, in test_parse\n",
      "    self.assertEqual(parse(path), expected)\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 17, in parse\n",
      "    headers_first = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
      "NameError: name 'body' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_parse (__main__.TestParse) (path='wiki/Python_(programming_language)', expected=[2, 5, 17, 41])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 53, in test_parse\n",
      "    self.assertEqual(parse(path), expected)\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 17, in parse\n",
      "    headers_first = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
      "NameError: name 'body' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_parse (__main__.TestParse) (path='wiki/Spectrogram', expected=[1, 2, 4, 7])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 53, in test_parse\n",
      "    self.assertEqual(parse(path), expected)\n",
      "  File \"<ipython-input-3-0765cb173301>\", line 17, in parse\n",
      "    headers_first = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
      "NameError: name 'body' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 13.408s\n",
      "\n",
      "FAILED (errors=10)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import unittest\n",
    "\n",
    "def parse(path_to_file):\n",
    "    with open(path_to_file, encoding='utf-8') as f:\n",
    "        html = f.read()\n",
    "        \n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    root = soup.find('div', id='bodyContent')\n",
    "        \n",
    "    imgs = len(root.find_all(\n",
    "        lambda tag:tag.name == \"img\" and\n",
    "        \"width\" in tag.attrs and \n",
    "        int(tag[\"width\"]) >= 200\n",
    "    ))\n",
    "    \n",
    "    headers_first = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    headers_first = [header.text for header in headers_first]\n",
    "    headers = []\n",
    "    for i in range(len(headers_first)):\n",
    "        if headers_first[i][0] in ['E','T','C']:\n",
    "            headers.append(headers_first[i])\n",
    "    headers = len(headers)\n",
    "    \n",
    "    lengths = []   \n",
    "    for link in root.find_all('a'):\n",
    "        counter = 1\n",
    "        for l in link.find_next_siblings():\n",
    "            if l.name  != 'a':\n",
    "                break\n",
    "            counter += 1\n",
    "        lengths.append(counter)\n",
    "    linkslen = max(lengths)\n",
    "        \n",
    "    lists = 0\n",
    "    for listitem in root.find_all(['ul', 'ol']):\n",
    "        if not listitem.find_parent('li'):\n",
    "            lists += 1                                 \n",
    "    \n",
    "    return [imgs, headers, linkslen, lists]\n",
    "\n",
    "class TestParse(unittest.TestCase):\n",
    "    def test_parse(self):\n",
    "        test_cases = (\n",
    "            ('wiki/Stone_Age', [13, 10, 12, 40]),\n",
    "            ('wiki/Brain', [19, 5, 25, 11]),\n",
    "            ('wiki/Artificial_intelligence', [8, 19, 13, 198]),\n",
    "            ('wiki/Python_(programming_language)', [2, 5, 17, 41]),\n",
    "            ('wiki/Spectrogram', [1, 2, 4, 7]),)\n",
    "\n",
    "        for path, expected in test_cases:\n",
    "            with self.subTest(path=path, expected=expected):\n",
    "                self.assertEqual(parse(path), expected)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # unittest.main()\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False) # Run unit-tests in Jupyter\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki/Stone_Age all nodes count 37\n",
      "wiki/Brain all nodes count 30\n",
      "wiki/Artificial_intelligence all nodes count 71\n",
      "wiki/Python_(programming_language) all nodes count 26\n",
      "wiki/The_New_York_Times all nodes count 52\n",
      "wiki/London all nodes count 60\n",
      "wiki/Woolwich all nodes count 38\n",
      "wiki/Iron_Age all nodes count 20\n",
      "wiki/Stone_Age all nodes count 37\n",
      "wiki/Artificial_intelligence all nodes count 71\n",
      "wiki/IBM all nodes count 14\n",
      "wiki/PlayStation_3 all nodes count 38\n",
      "wiki/Wild_Arms_(video_game) all nodes count 14\n",
      "wiki/Hidamari_no_Ki all nodes count 12\n",
      "wiki/Mei_Kurokawa all nodes count 3\n",
      "wiki/The_New_York_Times all nodes count 52\n",
      "wiki/Second_Intifada all nodes count 40\n",
      "wiki/Haifa_bus_16_suicide_bombing all nodes count 6\n",
      "wiki/Binyamina_train_station_suicide_bombing all nodes count 6\n",
      "wiki/Stone_Age all nodes count 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FAIL: test_build_bridge (__main__.TestGetStatistics) (path='wiki/', start_page='The_New_York_Times', end_page='Stone_Age', expected=['The_New_York_Times', 'London', 'Woolwich', 'Iron_Age', 'Stone_Age'])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 113, in test_build_bridge\n",
      "    self.assertEqual(result, {page: STATISTICS[page] for page in expected})\n",
      "AssertionError: {'The[17 chars] [5, 8, 8, 42], 'London': [53, 16, 31, 125], '[81 chars] 40]} != {'The[17 chars] [5, 9, 8, 42], 'London': [53, 16, 31, 125], '[81 chars] 40]}\n",
      "  {'Iron_Age': [4, 8, 15, 22],\n",
      "   'London': [53, 16, 31, 125],\n",
      "   'Stone_Age': [13, 10, 12, 40],\n",
      "-  'The_New_York_Times': [5, 8, 8, 42],\n",
      "?                              ---\n",
      "\n",
      "+  'The_New_York_Times': [5, 9, 8, 42],\n",
      "?                            +++\n",
      "\n",
      "   'Woolwich': [15, 9, 19, 38]}\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_build_bridge (__main__.TestGetStatistics) (path='wiki/', start_page='The_New_York_Times', end_page='Binyamina_train_station_suicide_bombing', expected=['The_New_York_Times', 'Second_Intifada', 'Haifa_bus_16_suicide_bombing', 'Binyamina_train_station_suicide_bombing'])\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-d4529b540632>\", line 113, in test_build_bridge\n",
      "    self.assertEqual(result, {page: STATISTICS[page] for page in expected})\n",
      "AssertionError: {'The[17 chars] [5, 8, 8, 42], 'Second_Intifada': [9, 13, 14,[106 chars] 21]} != {'The[17 chars] [5, 9, 8, 42], 'Second_Intifada': [9, 13, 14,[106 chars] 21]}\n",
      "  {'Binyamina_train_station_suicide_bombing': [1, 3, 6, 21],\n",
      "   'Haifa_bus_16_suicide_bombing': [1, 4, 15, 23],\n",
      "   'Second_Intifada': [9, 13, 14, 84],\n",
      "-  'The_New_York_Times': [5, 8, 8, 42]}\n",
      "?                              ---\n",
      "\n",
      "+  'The_New_York_Times': [5, 9, 8, 42]}\n",
      "?                            +++\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 15.749s\n",
      "\n",
      "FAILED (failures=2)\n"
     ]
    }
   ],
   "source": [
    "# Набор тестов для проверки студентами решений по заданию \"Практическое задание\n",
    "# по Beautiful Soup - 2\". По умолчанию файл с решением называется solution.py,\n",
    "# измените в импорте название модуля solution, если файл с решением имеет другое имя.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unittest\n",
    "from collections import deque\n",
    "from solution_part1 import parse\n",
    "import itertools\n",
    "\n",
    "\n",
    "# from solution import build_bridge, get_statistics\n",
    "\n",
    "STATISTICS = {\n",
    "    'Artificial_intelligence': [8, 19, 13, 198],\n",
    "    'Binyamina_train_station_suicide_bombing': [1, 3, 6, 21],\n",
    "    'Brain': [19, 5, 25, 11],\n",
    "    'Haifa_bus_16_suicide_bombing': [1, 4, 15, 23],\n",
    "    'Hidamari_no_Ki': [1, 5, 5, 35],\n",
    "    'IBM': [13, 3, 21, 33],\n",
    "    'Iron_Age': [4, 8, 15, 22],\n",
    "    'London': [53, 16, 31, 125],\n",
    "    'Mei_Kurokawa': [1, 1, 2, 7],\n",
    "    'PlayStation_3': [13, 5, 14, 148],\n",
    "    'Python_(programming_language)': [2, 5, 17, 41],\n",
    "    'Second_Intifada': [9, 13, 14, 84],\n",
    "    'Stone_Age': [13, 10, 12, 40],\n",
    "    'The_New_York_Times': [5, 9, 8, 42],\n",
    "    'Wild_Arms_(video_game)': [3, 3, 10, 27],\n",
    "    'Woolwich': [15, 9, 19, 38]}\n",
    "\n",
    "TESTCASES = (\n",
    "    ('wiki/', 'Stone_Age', 'Python_(programming_language)',\n",
    "     ['Stone_Age', 'Brain', 'Artificial_intelligence', 'Python_(programming_language)']),\n",
    "\n",
    "    ('wiki/', 'The_New_York_Times', 'Stone_Age',\n",
    "     ['The_New_York_Times', 'London', 'Woolwich', 'Iron_Age', 'Stone_Age']),\n",
    "\n",
    "    ('wiki/', 'Artificial_intelligence', 'Mei_Kurokawa',\n",
    "     ['Artificial_intelligence', 'IBM', 'PlayStation_3', 'Wild_Arms_(video_game)',\n",
    "      'Hidamari_no_Ki', 'Mei_Kurokawa']),\n",
    "\n",
    "    ('wiki/', 'The_New_York_Times', \"Binyamina_train_station_suicide_bombing\",\n",
    "     ['The_New_York_Times', 'Second_Intifada', 'Haifa_bus_16_suicide_bombing',\n",
    "      'Binyamina_train_station_suicide_bombing']),\n",
    "\n",
    "    ('wiki/', 'Stone_Age', 'Stone_Age',\n",
    "     ['Stone_Age', ]),\n",
    ")\n",
    "\n",
    "\n",
    "def build_bridge(path, start_page, end_page):\n",
    "    \n",
    "    graph = {}\n",
    "    dist = {start_page: [start_page]}\n",
    "    q = deque([start_page])\n",
    "    while len(q):\n",
    "        at = q.popleft()        \n",
    "        fpath = os.path.join(path, at)\n",
    "        if not os.path.isfile(fpath):\n",
    "            continue\n",
    "        with open(fpath, encoding=\"utf-8\") as file:\n",
    "            links = re.findall(r\"(?<=/wiki/)[\\w()]+\", file.read())\n",
    "            graph[at] = links\n",
    "        for next in graph[at]:\n",
    "            if next not in dist:\n",
    "                dist[next] = [dist[at], next]\n",
    "                q.append(next)\n",
    "                \n",
    "    result1 = dist.get(end_page)\n",
    "    result = [] \n",
    "    def reemovNestings(l): \n",
    "        for i in l: \n",
    "            if type(i) == list: \n",
    "                reemovNestings(i) \n",
    "            else: \n",
    "                result.append(i) \n",
    "    reemovNestings(result1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_statistics(path, start_page, end_page):    \n",
    "    result = {}\n",
    "    \n",
    "    for node in build_bridge(path, start_page, end_page):\n",
    "        fpath = os.path.join(path, node)\n",
    "        stats = parse(fpath)\n",
    "        result[node] = stats\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "class TestBuildBrige(unittest.TestCase):\n",
    "    def test_build_bridge(self):\n",
    "        for path, start_page, end_page, expected in TESTCASES:\n",
    "            with self.subTest(path=path,\n",
    "                              start_page=start_page,\n",
    "                              end_page=end_page,\n",
    "                              expected=expected):\n",
    "                result = build_bridge(path, start_page, end_page)\n",
    "                self.assertEqual(result, expected)\n",
    "\n",
    "\n",
    "class TestGetStatistics(unittest.TestCase):\n",
    "    def test_build_bridge(self):\n",
    "        for path, start_page, end_page, expected in TESTCASES:\n",
    "            with self.subTest(path=path,\n",
    "                              start_page=start_page,\n",
    "                              end_page=end_page,\n",
    "                              expected=expected):\n",
    "                result = get_statistics(path, start_page, end_page)\n",
    "                self.assertEqual(result, {page: STATISTICS[page] for page in expected})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # unittest.main()\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False) # Run unit-tests in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
